# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zHrZMDH3DY6gojTOatZuJofUOmtW2Kwy
"""

# !pip install -U langchain-community

# pip install accelerate bitsandbytes

# from google.colab import drive
# drive.mount('/content/drive')

# pip install faiss-cpu

# pip install gradio

# pip install pytesseract PyPDF2

"""#making an app"""

import torch
import math
import gradio as gr
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline


# 1. Device setup
DEVICE = 0 if torch.cuda.is_available() else -1
print(f"üîç Using device: {'GPU' if DEVICE==0 else 'CPU'}")

# 2. Load your fine‚Äëtuned model & tokenizer
SAVE_DIR = "saved_model_summary"
tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)
model     = AutoModelForSeq2SeqLM.from_pretrained(SAVE_DIR).to(0 if DEVICE==0 else torch.device("cpu"))

# 3. Build HuggingFace pipeline
summarizer = pipeline(
    "summarization",
    model=model,
    tokenizer=tokenizer,
    device=DEVICE
)

# 4. Helper functions
import math

def split_into_chunks(
    text: str,
    chunk_size: int = 200,
    min_last_chunk: int = 100,
    word_overlap: int = 30
):
    """
    Splits text into word‚Äëbased chunks of up to `chunk_size` with `word_overlap` between chunks.
    If the final chunk is smaller than `min_last_chunk`, it merges it into the previous chunk.
    """
    words = text.split()
    chunks = []
    i = 0

    while i < len(words):
        # Get the chunk
        chunk = words[i:i + chunk_size]

        # If there's an existing chunk and this one is "too small", merge it
        if chunks and len(chunk) < min_last_chunk:
            chunks[-1].extend(chunk)
            break

        chunks.append(chunk)
        # Move to next chunk with overlap
        i += chunk_size - word_overlap

    # Convert list of word‚Äëlists back to strings
    return [" ".join(chunk) for chunk in chunks]



def generate_summary(
    text: str,
    chunk_size: int = 200,
    max_len: int = 125,
    min_len: int = 60
) -> str:
    # Break text into smart chunks
    chunks = split_into_chunks(
        text,
        chunk_size=chunk_size,
        min_last_chunk=100  # change this threshold as needed
    )

    summaries = []
    for idx, chunk in enumerate(chunks, start=1):
        inp = chunk if chunk.lower().startswith("summarize:") else "summarize: " + chunk
        out = summarizer(
            inp,
            max_length=max_len,
            min_length=min_len,
            do_sample=False
        )[0]["summary_text"]
        summaries.append(out)

    # Concatenate all partial summaries
    return " ".join(summaries)

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
# QA LLM
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from langchain.prompts import PromptTemplate

# Configuratio
QA_MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"  
AUTH_TOKEN = "Your API KEY"  # Required for LLaMA models

# Quantization config for memory efficiency
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False
)

# Load LLaMA model with quantization
qa_model = AutoModelForCausalLM.from_pretrained(
    QA_MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    use_auth_token=AUTH_TOKEN
)

# Load tokenizer
qa_tok = AutoTokenizer.from_pretrained(
    QA_MODEL_NAME,
    use_auth_token=AUTH_TOKEN
)
qa_tok.pad_token = qa_tok.eos_token  # Set pad token

from transformers import pipeline
from langchain.llms import HuggingFacePipeline

qa_pipe = pipeline(
    "text-generation",
    model=qa_model,
    tokenizer=qa_tok,
    max_new_tokens=512,
    temperature=0.5,
    top_p=0.95,
    repetition_penalty=1.2
)

llm = HuggingFacePipeline(pipeline=qa_pipe)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Custom prompt template
LLAMA_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template="""[INST] <<SYS>>
    You are a helpful assistant that answers questions based on context.
    <</SYS>>

    Context: {context}

    Question: {question}

    Answer: [/INST]"""
)

vectorstore = None
qa_chain = None

def prepare_qa_chain(text):
    global vectorstore, qa_chain
    splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)
    docs = splitter.split_text(text)
    vectorstore = FAISS.from_texts(docs, embeddings)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": LLAMA_PROMPT}
    )
    return qa_chain

import re

def clean_answer(raw: str) -> str:
    # 1) Remove everything up through ‚ÄúAnswer:‚Äù (including the marker itself)
    cleaned = re.sub(r'(?s)^.*?Answer:\s*', '', raw)
    # 2) Remove any trailing [/INST] and beyond
    cleaned = re.sub(r'\[/INST\].*$', '', cleaned)
    # 3) Trim whitespace
    cleaned = cleaned.strip()
    # 4) If we accidentally stripped everything, return the original
    return cleaned or raw.strip()

def answer_question(user_question: str) -> str:
    global qa_chain
    if qa_chain is None:
        return "‚ö†Ô∏è Please input and process some text first using 'Build Chatbot'."
    if not user_question.strip():
        return "‚ö†Ô∏è Please enter a question."

    raw = qa_chain.run(user_question)
    return clean_answer(raw)

from PIL import Image
import pytesseract
from PyPDF2 import PdfReader

def extract_text_from_pdf(file):
    reader = PdfReader(file.name)
    text = ''
    for page in reader.pages:
        text += page.extract_text() or ''
    return text


def extract_text_from_image(file):
    img = Image.open(file.name)
    return pytesseract.image_to_string(img)

def build_ui():
    custom_css = """
    .gradio-container {
        background: #454545;
        font-family: 'Segoe UI', sans-serif;
    }
    .gr-button {
        background-color: #4a90e2 !important;
        color: white !important;
        border-radius: 8px !important;
    }
    .gr-textbox textarea {
        background-color: #ffffff;
        border: 1px solid #ccc;
        border-radius: 6px;
    }
    input[type="file"] {
        background-color: #e0ecff;
        border: 2px dashed #4a90e2;
        padding: 10px;
        border-radius: 10px;
        font-family: 'Segoe UI', sans-serif;
        height: 20px !important;
        padding: 2px 6px !important;
        font-size: 0.85rem !important;
        line-height: 1 !important;
    }
    .gr-Chatbot textarea{
        height:200px;
        background-color: #e0ecff;
        border: 2px dashed #4a90e2;
        padding: 10px;
        border-radius: 10px;
        font-family: 'Segoe UI', sans-serif;
    }
    """

    with gr.Blocks(css=custom_css) as demo:
        gr.Markdown("## üìù AI Summarizer & üìö QA Chatbot", elem_id="header")

        with gr.Row():
            with gr.Column(scale=1):
                input_text = gr.Textbox(label="‚úèÔ∏è Enter Text", placeholder="Paste or type your text here...", lines=8)
                image_input = gr.File(label="üñºÔ∏è Upload Image (OCR)", file_types=["image"])
                pdf_input = gr.File(label="üìÑ Upload PDF Document", file_types=[".pdf",'.txt','.docx'])
                summary_output = gr.Textbox(label="üìå Summary Output", lines=6)
                btn_summary = gr.Button("üîç Generate Summary")

            with gr.Column(scale=1):
                chatbot = gr.Chatbot(label="üí¨ QA Chat Interface", type="messages", height=400)
                question = gr.Textbox(label="‚ùì Ask a Question", placeholder="Type your question here...")
                btn_qa = gr.Button("üì® Send")

        def extract_text_from_pdf(file_path):
            """
            file_path may already be a string path.
            """
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() or ""
            return text

        def extract_text_from_image(file_path):
            """
            file_path may already be a string path.
            """
            img = Image.open(file_path)
            return pytesseract.image_to_string(img)

        def summary_handler(text, img, pdf):
            # Determine which input we got, and extract raw text
            if img:
                path = img.name if hasattr(img, "name") else img
                raw = extract_text_from_image(path)
            elif pdf:
                path = pdf.name if hasattr(pdf, "name") else pdf
                raw = extract_text_from_pdf(path)
            elif text:
                raw = text
            else:
                return "‚ö†Ô∏è Please provide input text, image, or PDF."

            # Initialize QA chain
            prepare_qa_chain(raw)

            # Return the summary
            return generate_summary(raw)


        btn_summary.click(
            fn=summary_handler,
            inputs=[input_text, image_input, pdf_input],
            outputs=[summary_output]
        )

        def chat_response(hist, q):
            if not q:
                return hist
            response = answer_question(q)
            hist.append({"role": "user", "content":q})
            hist.append({ "role": "assistant", "content":response})
            return hist

        btn_qa.click(
            fn=chat_response,
            inputs=[chatbot, question],
            outputs=[chatbot]
        )

    return demo

if __name__ == "__main__":
    demo = build_ui()
    demo.launch(debug=True)